\documentclass[a4paper]{article}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{float}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]

\title{Information Theory}
\author{listenzcc}

\begin{document}

\maketitle


\abstract
Not done yet.

\tableofcontents

\section{Information Theory}
Information Theory is one of the few scientific fields fortunate enough to have an identifiable beginning - Claude Shannon's 1948 paper.
\begin{quote}
    What made possible, what induced the development of coding as a theory, and
    the development of very complicated codes, was Shannon's Theorem: he told
    you that it could be done, so people tried to do it.
        [Interview with Fano, R. 2001]
\end{quote}

For a given distribution $X ~ P(X)$, a single symbol $x$ can explain the amount of uncertainty as the information quantity
\begin{equation}
    I(x) = \log{\frac{1}{p(x)}} = - \log{p(x)}
    \label{eq: Information Quantity x}
\end{equation}
where the base of $\log{}$ is arbitrary. Usually we use $2$ as the base, and unit of the information quantity is called $bytes$.\footnote{The $\log{}$ in this manuscript uses the base of $2$ if not specified.}

The expectation of the mean of the information quantity is information entropy (or Shannon entropy)
\begin{equation}
    H(X) = \int_{X} p(x) I(x) dx
    \label{eq: Information Entropy X}
\end{equation}

\begin{theorem}
    The information entropy is maximized when all the symbols occurs in equal probabilities.
    In a discrete situation, $X$ has $n$ possible values. When $p(x) = \frac{1}{n}$, the information entropy is maximized.
\end{theorem}

\begin{proof}
    Re-write information entropy as
    \begin{equation*}
        H(x) = - \mathcal{C} \sum_{i=1}^{n} p(x_{i}) \ln{p(x_{i})}
    \end{equation*}
    where $\mathcal{C}$ is a constant which guarantee $\mathcal{C} \ln{p} = \log{p}$ when $0<p<1$.
    To maximizing the information entropy, there is another constraint that $\sum_{i=1}^{n} p(x) = 1$.

    Use Lagrangian method to solve the constrained maximizing problem.
    Formulate Lagrangian function
    \begin{equation*}
        \mathcal{L}(x) = H(x) + \lambda(\sum_{i=1}^{n} p(x_{i}) - 1)
    \end{equation*}
    where $\lambda$ is unsolved constant.

    Calculate the partial differential of $\mathcal{L}(x)$ to $p(x_{i})$
    \begin{equation*}
        \frac{\partial}{\partial{p(x_{i})}} {H} = \lambda - \mathcal{C} \ln{p(x_{i})} + \mathcal{C}
    \end{equation*}
    the maximizing of information entropy is equivalent to the partial differentials equal to zero for each $i \in [1, 2, \dots, n]$.

    Since $\lambda$ is constant, we have
    \begin{equation*}
        p(x_{i}) = p(x_{j}) = p(x)
    \end{equation*}
    for each $i \neq j$ and $i, j \in [1, 2, \dots, n]$.
    Hence proved.
\end{proof}

In the sense of above analysis, we can see that the information entropy can be considered as the minimized code length of a communication system.
To make sure the system reaches the minimized code length, an efficient way is to design it making sure all the symbols are happening with equal possibility.
Here is another question to be answered: how many symbols do we have to use in the system?

\begin{theorem}
    The best number of symbols in a equal possibility system is $e$.
    The value can maximize the information quantity of a single symbol.
\end{theorem}

\begin{proof}
    Re-write the information entropy in a equal possibility discrete system.
    \begin{equation*}
        H(x) = - \mathcal{C} \sum_{i=1}^{n} p \ln{p}
    \end{equation*}
    where $p = p(x_{i}) = \frac{1}{n}$ for $i \in [1, 2, \dots, n]$.

    Since all the symbols have the same probability, the information quantity of a single symbol can be wrote as
    \begin{equation*}
        I = \mathcal{C} \frac{1}{n} \ln{n}
    \end{equation*}

    Calculate the partial derivative by $n$, we have
    \begin{equation*}
        \frac{\partial}{\partial{n}} I = \mathcal{C} (\frac{1}{n^{2}} - \frac{1}{n^{2}} \ln{n})
    \end{equation*}
    one can see that $n=e$ can make $\frac{\partial}{\partial{n}} I = 0$, and the $2^{nd}-order$ partial is negative when $n=e$.
    It turns out that the value maximizes the information being carried by single symbol.
    Hence proved.
\end{proof}

\section{Mutual Information}

In practice, one may concerns the interaction between several variables.
We can start with two variables.
The simplest situation is that two variables are independent with each other.

\paragraph{Independent variables}

If $X$ and $Y$ are independent with each other, the joint probability can be expressed as
\begin{equation}
    P(X, Y) = P(X) P(Y)
    \label{eq: Joint Probability of Independent Variables}
\end{equation}
which is a necessary condition of independence, although not sufficient.

\begin{theorem}
    The information entropy of independent variables equals to the summation of each information entropy.
\end{theorem}

\begin{proof}
    The information entropy of $X$ and $Y$ can be expressed as
    \begin{equation}
        H(X, Y) = - \int_{X} \int_{Y} P(x, y) \log(P(x, y)) dx dy
    \end{equation}
    use (\ref{eq: Joint Probability of Independent Variables}), we have
    \begin{equation}
        H(X, Y) = - \int_{X} P(x) \log(P(x)) dx - \int_{Y} P(y) \log(P(y)) dy
    \end{equation}
    the equation also uses the fact that $\int_{Y} P(x, y) dy = P(x)$.
    Use the definition in (\ref{eq: Information Entropy X}) we have
    \begin{equation*}
        H(X, Y) = H(X) + H(Y)
    \end{equation*}

    Hence proved.
\end{proof}

\paragraph{Dependent variables}
If $X$ and $Y$ are not independent, the mutual information can be expressed as
\begin{equation}
    I(X; Y) = \int_{X} \int_{Y} p(x, y) \log{\frac{p(x, y)}{p(x)p(y)} dx dy}
    \label{eq: Multual Information}
\end{equation}

The meaning of mutual information is the uncertainty of one variable solved by another variable.

\begin{theorem}
    The mutual information is symmetrical
    \begin{equation}
        I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
    \end{equation}
\end{theorem}

\begin{proof}
    Re-write (\ref{eq: Multual Information}) we have
    \begin{equation*}
        I(X; Y) = \int_{X} \int_{Y} p(x, y) \log{p(x, y)} dx dy - \int_{X} p(x) \log{p(x)} dx - \int_{Y} p(y) \log{p(y) dy}
    \end{equation*}

    Start with $H(Y|X)$, it is the information entropy of conditional probability.
    \begin{equation*}
        H(Y|X) = - \int_{X} \int_{Y} p(y, x) \log{p(y|x)} dx dy
    \end{equation*}
    calculate further
    \begin{equation*}
        H(Y|X) = - \int_{X} \int_{Y} p(y, x) \log{p(y, x)} dx dy + \int_{X} p(x) \log{p(x)} dx
    \end{equation*}

    Easy to obtain
    \begin{equation*}
        H(Y|X) + I(X; Y) = H(Y)
    \end{equation*}

    It is the same if we start with $H(X|Y)$, it will yield $H(X|Y) + I(X; Y) = H(X)$.
    Hence proved.
\end{proof}

\end{document}
