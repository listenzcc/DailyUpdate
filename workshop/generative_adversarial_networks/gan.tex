\documentclass[a4paper]{article}

\usepackage{amssymb}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}

\title{Generative Adversarial Networks}
\author{listenzcc}

\begin{document}

\maketitle

\abstract
Not done yet.

\tableofcontents

\section{Real and Fake samples}
Generative Adversarial Networks (GAN) can generate \emph{FAKE} samples using adversarial learning algorithm.
GAN has been widely used for extending new samples or stylize a given sample.
The aim is to make the fake samples following the same distribution with \emph{REAL} samples.

GAN has two parts, \emph{Generator} ($G$) and \emph{Discriminator} ($D$).
The generator is to generate \emph{FAKE} samples, the discriminator is to detect them.
Thus, $G$ and $D$ are adversarial.
The question is how GAN works.

\section{Discriminator}

\subsection{Definition}

A \emph{Discriminator} is a two-classes classifier $D(x)$ that gives $1$ for \emph{REAL} sample and $0$ for \emph{FAKE} one.
Thus, the loss function of a discriminator is
\begin{equation} \label{eq:D_loss}
    - \mathbb{E}_{x\sim P_r} [\log{D(x)}]
    - \mathbb{E}_{x\sim P_g} [\log{(1-D(x))}]
\end{equation}
where $P_r$ and $P_g$ are the probability of an image $x$ belongs to \emph{REAL} and \emph{FAKE} distributions.

To a given generator, the loss caused by an image is
\begin{equation}
    \mathcal{L}{(x)} =
    - P_r(x) \log{D(x)}
    - P_g(x) \log{(1-D(x))}
\end{equation}

\subsection{Optimal discriminator}

\begin{theorem}
    The optimal discriminator of a given image $x$ is
    \begin{equation} \label{eq:Optimal_D}
        D^*(x) = \frac{P_r(x)}{P_r(x) + P_g(x)}
    \end{equation}

    \begin{proof}
        Note the loss function as $\mathcal{L}$, we have
        \begin{equation*}
            \begin{aligned}
                \frac{\partial}{\partial D} \mathcal{L}     & =
                - \frac{P_r(x)}{D(x)}
                + \frac{P_g(x)}{1-D(x)}                         \\
                \frac{\partial}{\partial^2 D} \mathcal{L}^2 & =
                \frac{P_r(x)}{D^2(x)}
                - \frac{P_g(x)}{(1-D(x))^2}
            \end{aligned}
        \end{equation*}
        One solution that minimizes the $\mathcal{L}$ is
        \begin{equation*}
            D^*(x) = \frac{P_r(x)}{P_r(x) + P_g(x)}
        \end{equation*}
        when $P_r(x) \leqslant P_g(x)$, we have $\frac{\partial}{\partial^2 D} \mathcal{L}^2 \geqslant 0$.
        Which guarantees that $D^*(x)$ is the minimization solution.

        Hence proved.
    \end{proof}
\end{theorem}

\section{Generator}

\subsection{Definition}

The loss function of generator can be like
\begin{equation} \label{eq:G_loss0}
    \mathbb{E}_{x\sim P_g} [\log{(1-D(x))}]
\end{equation}
or
\begin{equation} \label{eq:G_loss1}
    \mathbb{E}_{x\sim P_g} [\log{(-D(x))}]
\end{equation}
the aim is to deceive the discriminator makes $D(x) = 1$ when $x \sim P_g$.

\subsection{Loss function under optimal discriminator}

The relationship between loss function and $KL$ and $JS$ divergence is rather close.

\begin{theorem} \label{th:JS_Gloss}
    Under optimal discriminator $D^*(x)$, the (\ref{eq:G_loss0}) equals to $JS$ divergence plus a constant.
    \begin{equation*}
        \mathbb{E}_{x\sim P_g} [\log{(1-D^*(x))}]
        = 2 JS(P_r \Vert P_g) - 2 \log{2}
        - \mathbb{E}_{x\sim P_r} [\log{D^*(x)}]
    \end{equation*}

    \begin{proof}
        Start by defining two measurements, $KL$ divergence and $JS$ divergence.
        \begin{equation} \label{eq:KL_JS}
            \begin{aligned}
                KL(P_1 \Vert P_2)   & = \mathbb{E}_{x\sim P_1} \log{\frac{P_1}{P_2}}                      \\
                2 JS(P_1 \Vert P_2) & = KL(P_1 \Vert \frac{P_1+P_2}{2}) + KL(P_2 \Vert \frac{P_1+P_2}{2})
            \end{aligned}
        \end{equation}

        Recall (\ref{eq:Optimal_D}) we have
        \begin{equation*}
            \begin{aligned}
                \mathbb{E}_{x\sim P_r}[\log{D^*(x)}]     & = \mathbb{E}_{x\sim P_r}[\log{\frac{P_r(x)}{P_r(x)+ P_g(x)}}] \\
                \mathbb{E}_{x\sim P_g}[\log{(1-D^*(x))}] & = \mathbb{E}_{x\sim P_g}[\log{\frac{P_g(x)}{P_r(x)+ P_g(x)}}]
            \end{aligned}
        \end{equation*}

        Use (\ref{eq:KL_JS}) we have
        \begin{equation*}
            \begin{aligned}
                \mathbb{E}_{x\sim P_r}[\log{D^*(x)}]     & = KL(P_r \Vert \frac{P_r+P_g}{2}) - \log{2} \\
                \mathbb{E}_{x\sim P_g}[\log{(1-D^*(x))}] & = KL(P_g \Vert \frac{P_r+P_g}{2}) - \log{2}
            \end{aligned}
        \end{equation*}

        Adding above equations leads to
        \begin{equation*}
            \mathbb{E}_{x\sim P_r}[\log{D^*(x)}] + \mathbb{E}_{x\sim P_g}[\log{(1-D^*(x))}]
            = 2 JS(P_r \Vert P_g) - 2 \log{2}
        \end{equation*}

        Hence proved.
    \end{proof}

\end{theorem}

\begin{theorem} \label{th:KL_Gloss}
    Under optimal discriminator $D^*(x)$, the (\ref{eq:G_loss0}) and (\ref{eq:G_loss1}) are related by $KL$ divergence.
    \begin{equation*}
        \mathbb{E}_{x\sim P_g}[\log {(-D^*(x))}]
        = KL(P_g \Vert P_r)
        - \mathbb{E}_{x\sim P_g}[\log {(1-D^*(x))}]
    \end{equation*}

    \begin{proof}
        \begin{equation*}
            \begin{aligned}
                KL(P_g \Vert P_r) & = \mathbb{E}_{x\sim P_g}[\log {\frac{P_g(x)}{P_r(x)}}]   \\
                                  & = \mathbb{E}_{x\sim P_g}[\log {\frac{1-D^*(x)}{D^*(x)}}] \\
                                  & = \mathbb{E}_{x\sim P_g}[\log {1-D^*(x)}]
                - \mathbb{E}_{x\sim P_g}[\log {D^*(x)}]
            \end{aligned}
        \end{equation*}

        Hence proved.
    \end{proof}

\end{theorem}

Use Theorem \ref{th:JS_Gloss} we can conclude that under optimized discriminator, the training of the generator equals to minimize the JS divergence between $P_r$ and $P_g$.

\subsection{Gradient vanishing}
In high dimensional space, where the support set of the data \emph{MANIFOLD} is smaller than the space.
The $JS$ divergence is $0$ at almost for all the images.
It results that the metric is almost $0$
\begin{equation}
    \int P_r(x) P_g(x) dx \approx 0
\end{equation}
it shows that either $P_r$ or $P_g$ is $0$ for almost every image in the space.
It makes $JS$ divergence drops to $0$, which means gradient vanishes.
As a result, the gradient of (\ref{eq:G_loss0}) is vanishing under $D^*$.

\subsection{Log D trick}
One solution to gradient vanishing is $\log D$ trick.
It changes the (\ref{eq:G_loss0}) into (\ref{eq:G_loss1}).

\begin{theorem}
    Minimizing (\ref{eq:G_loss1}) under optimized discriminator $D^*(x)$ is equals to minimizing following
    \begin{equation}
        KL(P_g \Vert P_r) - 2 JS(P_r \Vert P_g)
    \end{equation}

    \begin{proof}
        Combining theorem \ref{th:JS_Gloss}, theorem \ref{th:KL_Gloss} and (\ref{eq:G_loss1}), we can proof.

        Begin with theorem \ref{th:KL_Gloss},
        \begin{equation*}
            \mathbb{E}_{x\sim P_g}[\log {(-D^*(x))}]
            = KL(P_g \Vert P_r)
            - \mathbb{E}_{x\sim P_g}[\log {(1-D^*(x))}]
        \end{equation*}

        Use theorem \ref{th:JS_Gloss},
        \begin{equation*}
            \begin{aligned}
                \mathbb{E}_{x\sim P_g}[\log {(-D^*(x))}]
                 & = KL(P_g \Vert P_r) - 2 JS(P_r \Vert P_g)                     \\
                 & \qquad + 2 \log{2} + \mathbb{E}_{x\sim P_r[\log {(-D^*(x))}]}
            \end{aligned}
        \end{equation*}

        It is obvious that the latter two factors is irrelevant with the generator.

        Hence proved.
    \end{proof}

\end{theorem}

It results a conflict that the optimization process requires $KL$ divergence to be smaller when $JS$ divergence to be larger at the same time.
Thus, it causes unstable of the learning process.

\subsection{Instable}
The instability is mainly because of the asymmetric of the $KL$ divergence.
Recall the definition of $KL$ divergence (\ref{eq:KL_JS}), we have two different situations:


\item ZERO:
When $P_g(x) \rightarrow 0$ and $P_r(x) \rightarrow 1$, we have
\begin{equation*}
    P_g(x) \log \frac{P_g(x)}{P_r(x)} \rightarrow 0
\end{equation*}

\item INFINITY:
When $P_g(x) \rightarrow 1$ and $P_r(x) \rightarrow 0$, we have
\begin{equation*}
    P_g(x) \log \frac{P_g(x)}{P_r(x)} \rightarrow +\infty
\end{equation*}


\end{document}

